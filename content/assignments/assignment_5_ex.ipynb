{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Homework assignment 4 - Classification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Before you begin\n",
    "\n",
    "Remember to:\n",
    "\n",
    "1. Make your own copy of the notebook by pressing the \"Copy to drive\" button.\n",
    "2. Expend all cells by pressing **Ctrl+[**\n",
    "\n",
    "### Your IDs\n",
    "\n",
    "✍️ Fill in your IDs in the cell below:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "## Fill in your IDs (as a string)\n",
    "student1_id = '...'\n",
    "student2_id = '...'\n",
    "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "\n",
    "print('Hello ' + student1_id + ' & ' + student2_id)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tip of the day - Debugging with PDB (optional, for the advanced users):\n",
    "\n",
    "One of the main problems in working with Colab / Jupyter is the lack of a good debugger.\n",
    "\n",
    "A not so optimal solution (but sometimes better than nothing) is to use Python's built-in debugger. Python comes with a very basic command line based debugger called [PDB](https://docs.python.org/3/library/pdb.html). It has all the basic debugger capabilities but is missing a good interface.\n",
    "\n",
    "You can drop into debug mode in the middle of any Python code simply by placing the command **pdb.set_trace()** before the line which you want to debug (only after importing the **pdb** package). Once in debug mode, the debugger prompt will appear in which you can use the following commands:\n",
    "\n",
    "- __l__ or **list**: to print the current line and the surrounding code.\n",
    "- __n__ or **next**: to run the current line.\n",
    "- __c__ or **continue**: to continue running until a breakpoint or until the end of the function.\n",
    "- __q__ or **quit**: to stop the run and exit the debugger.\n",
    "- **b {number}**: Place a breakpoint in line {number}.\n",
    "- **!{python expression}**: to run any python expression.\n",
    "\n",
    "(This is only a partial list of the PDB's commands. For the full list of commands you can refer to the [official documentation](https://docs.python.org/2/library/pdb.html#debugger-commands), or at [this cheat sheet](https://kapeli.com/cheat_sheets/Python_Debugger.docset/Contents/Resources/Documents/index))\n",
    "\n",
    "Let us look at an example:\n",
    "\n",
    "- Add the line **pdb.set_trace()** to the following code just before the **x *= 4**  line, and execute the cell to drop into debug mode.\n",
    "- You should see a textbox next to a \"ipdb>\" prompt.\n",
    "- Type __l__ (followed by **Enter**) to print the current line and surrounding code.\n",
    "- Type **!print(x)** to print the value of the variable __x__ (you can also omit the print command and just use **!x** in this case).\n",
    "- Type **!x=2** to change the value of __x__.\n",
    "- Type **!print(x)** again.\n",
    "- Type __n__ execute the next line.\n",
    "- Type __l__ again.\n",
    "- Type **!print(x)** again.\n",
    "- Type **b 10** to place a breakpoint on the **return x** line.\n",
    "- Type __c__ to run the code until the breakpoint.\n",
    "- Type __l__ again.\n",
    "- Type **!print(x)** again.\n",
    "- Type __c__ or **q** to quit the debugger with or without finishing to run the code\n",
    "- After you finish playing with the debugger make sure you remove or comment out the **pdb.set_trace()** line.\n",
    "- Clear our all the debugger output (before submitting the code). You can do so, for example, by pressing the __X__ button left to the cell's output area."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import pdb\n",
    "\n",
    "def func(x):\n",
    "    x += 2\n",
    "    x *= 4\n",
    "    x -= 2\n",
    "    x /= 2\n",
    "    \n",
    "    return x\n",
    "\n",
    "print(func(3))"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### !! Important\n",
    "\n",
    "- The **pdb.set_trace()** command can be placed anywhere, but stepping through the code (using the **n** command) is only possible inside functions.\n",
    "- **You must exit the debugger** (using __c__ or **q**) in order to be able to run any other cells."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Importing Packages\n",
    "\n",
    "Importing the NumPy, Pandas and Matplotlib packages."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## This line makes matplotlib plot the figures inside the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "## Set some default values of the the matplotlib plots\n",
    "plt.rcParams['figure.figsize'] = (8.0, 8.0)  # Set default plot's sizes\n",
    "plt.rcParams['axes.grid'] = True  # Show grid by default in figures"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The tqdm Package - Adding  a Progress Bar\n",
    "\n",
    "When running a long calculation, we would usually want to have a progress bar to track the progress of our process. One great python package for creating such a progress bar is [**tqdm**](https://github.com/tqdm/tqdm). This package is easy to use and offers a highly customizable progress bar. \n",
    "\n",
    "For example, to add a progress bar to an existing loop, simply surrounding the iterable which the loops run over with the **tqdm.tqdm** command:\n",
    "\n",
    "```python\n",
    "import tqdm.notebook as tqdm\n",
    "for x in tqdm.tqdm(some_list):\n",
    "    some_long_running_function(x)\n",
    "```\n",
    "\n",
    "(The \"import tqdm.notebook as tqdm\" imports a version of tqdm which is fitted to run in a notebook).\n",
    "\n",
    "✍️ Add a progress bar to the following loop:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import tqdm.notebook as tqdm\n",
    "import time\n",
    "\n",
    "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "for ...\n",
    "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "    print('Step {}'.format(i))\n",
    "    time.sleep(1)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Graphical Processing Unit (GPU)\n",
    "\n",
    "GPUs are special processing cards which were originally developed to for accelerating graphical related calculations such as 3D rendering and playing a high-resolution video. Today these cards are also in use for a variety of tasks which are not necessarily graphic related, such as training neural networks.\n",
    "\n",
    "These GPUs are optimal for running task which can be parallelized on a large amount of data. The CPU (Central Processing Unit) is the computer's main processing unit and usually has a smal number of fast and \"strong\" processing components called cores (today usually between 4 and 64). As opposed to it, a GPUs has many (usually thousands of) slower and \"weaker\" processing components.\n",
    "\n",
    "When running a process which performs some mathematical operation to a large amount of data, for example calculating the $e^x$ for each element in a large matrix or a multiplication between two matrices, we can speed up our process significantly by running it on a GPU.\n",
    "\n",
    "The GPU does not share the same memory space with the CPU and has its own memory. Therefore, before performing any calculation using a GPU, we must first transfer our data to the GPU's memory.\n",
    "\n",
    "### Colab and GPUs\n",
    "\n",
    "In this assignment, we will use a GPU to train our network. Colab offers free GPU support, but it is not enabled by default. To enable it, go to the menu bar, open **Runtime->Change runtime type** and change **hardware accelerator** to GPU. Click **save** to save your selection.\n",
    "\n",
    "In this assignment you will see how we can use a package called **PyTorch** in order to tell our code to perform an operation on the GPU instead of on the CPU. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PyTorch\n",
    "\n",
    "PyTorch is a framework (a collection of tools) which significantly simplify the process of building and training neural networks. This framework was initially developed and is currently maintained by Facebook. PyTorch is only one of many great such frameworks which currently exist. A list of some of the popular frameworks used today can be found [on Nvidia's site, here](https://developer.nvidia.com/deep-learning-frameworks).\n",
    "\n",
    "Specifically, in this assignment, we will relay on PyTorch for the following features:\n",
    "- The package's ability to automatic calculate gradients (using back-propagation)\n",
    "- The package's ability to move a variable to the GPU and perform calculations on it.\n",
    "- The package's stochastic gradient descent optimization object.\n",
    "- The built-in objects/function for building and training models:\n",
    "    - Linear layer\n",
    "    - Convolutional layers\n",
    "    - Relu\n",
    "    - SoftMax\n",
    "    - Minus-logLikelihood loss\n",
    " \n",
    "In this assignment, we will **not** cover all of what is needed for using PyTorch. It is aimed to show you the basics idea of what the framework has to offer. To better understand PyTorch, a good place to start are the great tutorials on the package's [website](https://pytorch.org/tutorials/index.html). The \"60 Minute Blitz\" along with the \"Learning PyTorch with Examples\" on the website provide a great starting point."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tensors\n",
    "\n",
    "The basic PyTorch object is the tensor with has a very similar (but not exact) interface to that of the NumPy array. A few differences which are worth mentioning:\n",
    "- Tensors do not yet support the \"@\" operator of performing matrix multiplication. It is performed by using  **torch.matmul(a_mat, b_mat)**.\n",
    "- The transpose of a matrix is given by  **a_mat.t()** (instead of the **a_mat.T** method which is in use in numpy)\n",
    "\n",
    "For example:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import torch  ## importing PyTorch\n",
    "\n",
    "## Defining a tensor from lists of numbers\n",
    "x1 = torch.tensor([[1.,2.], [3., 4.]])\n",
    "print('x1=\\n{}\\n'.format(x1))\n",
    "\n",
    "## Creating a random tensor\n",
    "x2 = torch.randint(low=0, high=10, size=(2, 3)).float()\n",
    "print('x2=\\n{}\\n'.format(x1))\n",
    "\n",
    "## Multipliing tensors\n",
    "y = torch.matmul(x1.t(), x2)\n",
    "print('torch.matmul(x1.t(), x2)=\\n{}'.format(y))"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### An Important Comment About Single & Double Precision & Fixed Points\n",
    "\n",
    "By default, numpy uses 64 bit to store floating point numbers. This representation is optimal for most CPUs. In contrast to that, PyTorch uses 32 bits, which is optimal for most GPUs. The 64-bit representation is called **double precision**, and the 32-bit is called **single precision**.\n",
    "\n",
    "Most of PyTorch's operations can only be performed only between two tensors of the same type. Therefore we will need to make sure that all of our tensors will be stored using single precision. You can convert a tensor to single precision representation by using the tensors **.float()** command.\n",
    "\n",
    "For some of the operations, we will also need to convert fixed point tensors (integers) to single precision. This is done in a similar way using the **.float()** command.\n",
    "\n",
    "For example:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "## This is a fixed point tensor\n",
    "x_int = torch.tensor([4, 2, 3])\n",
    "print('x_int=\\n{}'.format(x_int))\n",
    "print('x_int.dtype={}\\n'.format(x_int.dtype))\n",
    "\n",
    "## Converting the tensor to single persicion\n",
    "x_single = x_int.float()\n",
    "print('x_single=\\n{}'.format(x_single))\n",
    "print('x_single.dtype={}\\n'.format(x_single.dtype))\n",
    "\n",
    "## Converting the tensor to double persicion\n",
    "x_doubel = x_int.double()\n",
    "print('x_doubel=\\n{}'.format(x_doubel))\n",
    "print('x_doubel.dtype={}\\n'.format(x_doubel.dtype))"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### PyTorch and GPUs\n",
    "PyTorch provides a simple way to copy a tensor to the GPU's memory. The GPUs In PyTorch are referred to as [**CUDA**](https://en.wikipedia.org/wiki/CUDA) devices (which is the popular programing language for writing code for the GPU). A tensor can be copied to the GPU's memory by using the tensor's **.cuda** command. All the mathematical operations can then be performed on the copied tensor in the same way as if it was in the regular memory. The result of a calculation which was performed on the GPU will be stored on the GPU's memory as well.\n",
    "\n",
    "A tensor can be copied back from the GPU's memory to the regular memory using the **.cpu** command. For example:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "## Moving x1 to the GPU\n",
    "x1_gpu = x1.cuda()\n",
    "print('x1_gpu=\\n{}\\n'.format(x1_gpu))\n",
    "\n",
    "## Moving x2 to the GPU\n",
    "x2_gpu = torch.randint(low=0, high=10, size=(2, 3)).float().cuda()\n",
    "print('x2_gpu=\\n{}\\n'.format(x1_gpu))\n",
    "\n",
    "## Performing matrix multiplication on the GPU\n",
    "y = torch.matmul(x1_gpu.t(), x2_gpu)\n",
    "print('torch.matmul(x1_gpu.t(), x2_gpu)=\\n{}\\n'.format(y))\n",
    "\n",
    "print('y.cpu()=\\n{}'.format(y.cpu()))"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice the **device='cuda:0'** which is attached to the outputs of tensors which are stored on the GPU's memory."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "✍️ Calculate the multiplication table (לוח הכפל ($\\left[1,2,\\ldots,10\\right]^T\\left[1,2,\\ldots,10\\right]$)) on the GPU, copy the result back to the CPU and print the result:\n",
    "- PYTorch cannot multiply fixed point tensor on the GPU. Therefore so make sure you convert the tensors to single precision tensors."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "digits = torch.tensor([[1, 2, 3, 4, 5, 6, 7 ,8, 9, 10]]).float().cuda()\n",
    "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "mult_table = ...\n",
    "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "\n",
    "print(mult_table)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculating gradients\n",
    "\n",
    "One of PyTorch's main features is its ability to automatically calculate gradients by using back-propagation.\n",
    "\n",
    "To calculate the gradient of a function, we need to preforme the following steps:\n",
    "\n",
    "1. Select the variables according to which we would want to calculate the derivative.\n",
    "2. Clear all previous gradient calculations.\n",
    "3. Calculate the result of the functions for a given set of variables. (the forward path)\n",
    "4. Run the back-propagation function starting from the calculated result of the function.\n",
    "\n",
    "Let us start with an example, and then explain it.\n",
    "\n",
    "The following code calculates the following derivative: $\\left.\\frac{\\partial}{\\partial x}x^2+5x+4\\right|_{x=3}$:\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "## Define the variables which we would want to calculate the derivative according to\n",
    "x = torch.tensor(3).float()\n",
    "x.requires_grad = True\n",
    "\n",
    "## Calculate the function's result (forward pass)\n",
    "y = x ** 2 + 5 * x + 4\n",
    "\n",
    "## Run back-propagation\n",
    "y.backward()\n",
    "\n",
    "## Prin the result\n",
    "x_grad = x.grad\n",
    "print('The derivative is: {}'.format(x_grad))"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the above cell, we have performed the following steps:\n",
    "\n",
    "1. We have first defined a tensor **x**, and then marked be setting it's **.requires_grad** field to **True**. This tells PyTorch that we will later want to calculate the derivative according to it.\n",
    "2. We have calculated the function's result (this is the forward pass).\n",
    "3. We have used the result of the function to initiate the back-propagation calculation by using the **.backword()** function of the result tensor.\n",
    "\n",
    "After the back-propagation step, the derivative of the function according to each one of the selected variables will be stored in the **.grad** field of each of the variables.\n",
    "\n",
    "In this case, we did not have to clear any previous calculation since we did yet run any backward calculation using these variables."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "✍️ Calculate and plot the derivative of the sigmoid function $\\frac{1}{1+e^{-x}}$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "vals = np.arange(-10, 10, 0.1)\n",
    "res = np.zeros_like(vals)\n",
    "for i in range(len(vals)):\n",
    "    \n",
    "    x = torch.tensor(vals[i]).float()\n",
    "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "    ...\n",
    "    y = ...\n",
    "    ...\n",
    "    res[i] = ...\n",
    "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(vals, res);\n",
    "ax.set_title('$\\\\frac{\\\\partial}{\\\\partial x}\\\\frac{1}{1+e^{-x}}$', fontsize=20)\n",
    "ax.set_xlabel('$x$');"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the Digits Dataset\n",
    "\n",
    "We will use again the digits dataset from the last assignment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import load_digits\n",
    "dataset = load_digits()\n",
    "\n",
    "x = dataset.images\n",
    "y = dataset.target\n",
    "\n",
    "print('Number of images in the dataset: {}'.format(len(x)))\n",
    "print('Each images size is: {}'.format(x.shape[1:]))\n",
    "print('These are the first 80 images:')\n",
    "\n",
    "fig, ax_array = plt.subplots(10, 8)\n",
    "for i, ax in enumerate(ax_array.flat):\n",
    "    ax.imshow(x[i], cmap='gray')\n",
    "    ax.set_ylabel(y[i])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train-Validation-Test split\n",
    "\n",
    "✍️ Complete the code below to split the data into 60% train 20% validation set set and 20% test set similar to the last assignment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "n_samples = x.shape[0]  # The total number of samples in the dataset\n",
    "\n",
    "## Generate a random generator with a fixed seed\n",
    "rand_gen = np.random.RandomState(0)\n",
    "\n",
    "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "## Generating a shuffled vector of indices\n",
    "...\n",
    "\n",
    "## Split the indices into 80% train / 20% validation / 20% test\n",
    "...\n",
    "train_indices = ...\n",
    "val_indices = ...\n",
    "test_indices = ...\n",
    "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "\n",
    "## Extract the sub datasets from the full dataset using the calculated indices\n",
    "x_train = x[train_indices]\n",
    "y_train = y[train_indices]\n",
    "x_val = x[val_indices]\n",
    "y_val = y[val_indices]\n",
    "x_test = x[test_indices]\n",
    "y_test = y[test_indices]"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear Logistic Regression\n",
    "\n",
    "We will start by rewriting the linear logistic regression model from last assignment using PyTorch.\n",
    "\n",
    "**Reminder**: We are modeling the conditional distribution of the labels as:\n",
    "\n",
    "The way define and train models in PyTorch is by creating a class (which inherits the class of *torch.nn.Module*) and has two methods:\n",
    "\n",
    "- **\\_\\_init\\_\\_**: Which initialize the models layers with their parameters.\n",
    "- **forward**: which implements the model's functionality.\n",
    "\n",
    "We will implement the class using the following objects / functions from PyTorch:\n",
    "\n",
    "- [**torch.nn.Linear**](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear): defines a linear layer with its parameters, $\\Theta$. It is defined by the number of **input_features**, the number of **output_features**.\n",
    "- [**torch.nn.functional.log_softmax**](https://pytorch.org/docs/stable/nn.html#torch.nn.functional.log_softmax) which implements the log-softmax function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class LinearLogisticRegression(torch.nn.Module):\n",
    "     \n",
    "    def __init__(self, in_features, out_features):   \n",
    "        super(LinearLogisticRegression, self).__init__()\n",
    "        \n",
    "        ## Defining the linear function with it's parameters\n",
    "        ## =================================================\n",
    "        self.linear = torch.nn.Linear(in_features=in_features, out_features=out_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], x.shape[1] * x.shape[2])\n",
    "        z = self.linear(x)\n",
    "        y = torch.nn.functional.log_softmax(z, dim=1)\n",
    "        \n",
    "        return y"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mini-Batch Gradient Decent\n",
    "\n",
    "In order to run over the dataset using mini batches PyTorch offers the class of [**torch.utils.data.DataLoader**](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) which receives a PyTorch dataset and generates a series of batches running over the dataset.\n",
    "\n",
    "To generate a PyTorch dataset from a tensor of x and a tensor of y we will used the following command:\n",
    "\n",
    "```python\n",
    "torch.utils.data.TensorDataset(x, y)\n",
    "```\n",
    "\n",
    "We will convert x_train into a float tensor and y_train into a long integer tensor."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size = 64\n",
    "train_set = torch.utils.data.TensorDataset(torch.tensor(x_train).float(), torch.tensor(y_train).long())\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following function implement mini-batch gradient descent using:\n",
    "\n",
    "- The built-in optimization object [**torch.optim.SGD**](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) which performs gradient descent steps on a given set of parameters.\n",
    "- The built-in loss function [**torch.nn.functional.nll_loss**](https://pytorch.org/docs/stable/nn.html#torch.nn.functional.nll_loss) which calculates the negative log-likelihood of a matrix of log-probabilities, $P$, and a vector of labels, $\\boldsymbol{y}$ : $-\\frac{1}{N}\\sum_iP_{i,y_i}$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def train(model, eta, n_epochs, train_loader, x_val, y_val):\n",
    "    ## Move validation set to the GPU\n",
    "    x_val = x_val.cuda()\n",
    "    y_val = y_val.cuda()\n",
    "    \n",
    "    ## Initizalie the optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=eta)\n",
    "\n",
    "    ## Prepare lists to store intermediate obejectives\n",
    "    train_objective_list = [np.inf]\n",
    "    val_objective_list = [np.inf]\n",
    "    \n",
    "    ## Run for n_epochs\n",
    "    for epoch in tqdm.tqdm(range(n_epochs)):\n",
    "        ## Run over the batches\n",
    "        for x, y in train_loader:\n",
    "\n",
    "            ## Move batch to GPU\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            ## Forward pass\n",
    "            py_hat = model(x)\n",
    "            objective = torch.nn.functional.nll_loss(py_hat, y)\n",
    "            ## Backward pass\n",
    "            objective.backward()\n",
    "            ## Preform the gradient descent step\n",
    "            optimizer.step()\n",
    "    \n",
    "        ## Evaluate the objective on the validation set\n",
    "        with torch.no_grad(): ## This tell PyTorch not to calculate the gradients to save time\n",
    "            train_objective_list.append(objective.item())\n",
    "\n",
    "            py_hat = model(x_val)\n",
    "            objective = torch.nn.functional.nll_loss (py_hat, y_val)\n",
    "            val_objective_list.append(objective.item())\n",
    "\n",
    "    return train_objective_list, val_objective_list\n",
    "\n",
    "## Test the train function\n",
    "eta = 0.01\n",
    "n_epochs = 10\n",
    "model = LinearLogisticRegression(in_features=x_train.shape[1] * x_train.shape[2], out_features=10)\n",
    "model = model.cuda()\n",
    "train_objective_list, val_objective_list = train(model, eta, n_epochs, train_loader,\n",
    "                                                 torch.tensor(x_val).float(),\n",
    "                                                 torch.tensor(y_val).long())\n",
    "\n",
    "## Plot\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(np.arange(len(train_objective_list)), train_objective_list, label='Train')\n",
    "ax.plot(np.arange(len(val_objective_list)), val_objective_list, label='Validation')\n",
    "ax.set_title(r'$\\eta={' + f'{eta:g}' + r'}$')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Objective')\n",
    "ax.legend();"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Selecting the learning rate\n",
    "\n",
    "✍️ Complete the following code to plot the training figure (of the train and validation objectives over as a function of the number of steps) for the following learning rates: $[10^{-1},3\\cdot10^{-2},10^{-2},3\\cdot10^{-3}]$\n",
    "\n",
    "-Use: n_epochs = 100."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "n_epochs = 100\n",
    "etas_list = (1e-1, 3e-2, 1e-2, 3e-3)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(6, 6))\n",
    "for i_eta, eta in enumerate(etas_list):\n",
    "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "    model = ...\n",
    "    train_objective_list, val_objective_list = train(...\n",
    "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "    \n",
    "    ## Plot\n",
    "    ax = axes.flat[i_eta]\n",
    "    ax.plot(np.arange(len(train_objective_list)), train_objective_list, label='Train')\n",
    "    ax.plot(np.arange(len(val_objective_list)), val_objective_list, label='Validation')\n",
    "    ax.set_title(r'$\\eta={' + f'{eta:g}' + r'}$')\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Objective')\n",
    "axes[1,1].legend()\n",
    "fig.tight_layout()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Select the largest learning rate $\\eta$ in which the validation graph does not have large spikes (>0.1).\n",
    "\n",
    "✍️ Fill in the code below to select the learning rate and retrain the model for 500 epochs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "eta = ...\n",
    "n_epochs = ...\n",
    "model = ...\n",
    "train_objective_list, val_objective_list = train(...\n",
    "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "\n",
    "## Plot\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(np.arange(len(train_objective_list)), train_objective_list, label='Train')\n",
    "ax.plot(np.arange(len(val_objective_list)), val_objective_list, label='Validation')\n",
    "ax.set_title(r'$\\eta={' + f'{eta:g}' + r'}$')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Objective')\n",
    "ax.set_ylim(0, 0.5)\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Early Stopping\n",
    "\n",
    "We would like to stop the training when the validation objective reaches its minimum.\n",
    "\n",
    "✍️ Fill in the code below to rerun the training for the number of epoch which brings the validation objective to its minimum"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "optimal_number_of_steps = np.argmin(val_objective_list)\n",
    "print(optimal_number_of_steps)\n",
    "\n",
    "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "...\n",
    "train_objective_list, val_objective_list = ...\n",
    "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "\n",
    "## Plot\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(np.arange(len(train_objective_list)), train_objective_list, label='Train')\n",
    "ax.plot(np.arange(len(val_objective_list)), val_objective_list, label='Validation')\n",
    "ax.set_title(r'$\\eta={' + f'{eta:g}' + r'}$')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Objective')\n",
    "ax.set_ylim(0, 0.5);"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluating the Model on the Test Set\n",
    "\n",
    "The following code calculates the misclassification rate on the test set.\n",
    "\n",
    "In order to so we will define a data loader on the test set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "test_set = torch.utils.data.TensorDataset(torch.tensor(x_test).float(), torch.tensor(y_test).long())\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=batch_size)\n",
    "\n",
    "## Evaluate the score on the test set\n",
    "with torch.no_grad():\n",
    "    test_score_list = []\n",
    "    for x, y in test_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "        y_hat = model(x)\n",
    "        test_score = (torch.argmax(y_hat, dim=1) != y).sum()\n",
    "\n",
    "        test_score_list.append(test_score.item())\n",
    "\n",
    "test_score = np.sum(test_score_list) / len(test_set)\n",
    "\n",
    "print(f'The test score is: {test_score:.3}')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MLP\n",
    "\n",
    "Now that we have a training function and we know how to define models using PyTorch, we can start playing around with some neural-networks architectures. \n",
    "\n",
    "Specifically, we will run one MLP network and one CNN network.\n",
    "\n",
    "✍️ Complete the code below to define an MLP with 2 hidden layer of 1024 neurons and a ReLU activation function.\n",
    "\n",
    "I.e., build a network which with of the following layers:\n",
    "\n",
    "- **fc1**: A fully connected (linear) layer with an input of the in_features and output of 1024 with a ReLU activation.\n",
    "- **fc2**: A fully connected of 1024x1024 + ReLU.\n",
    "- **fc3**: A fully connected of 1024xout_features.\n",
    "- A softmax function\n",
    "\n",
    "In our case:\n",
    "\n",
    "- in_features = 64*64\n",
    "- out_features = 10\n",
    "\n",
    "We will use the function [**torch.nn.functional.relu**](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.relu) to implement the ReLU activation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "class MLP(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features):   \n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        ## Defining the fully connected (linear) layers with their parameters\n",
    "        ## ==================================================================\n",
    "        self.linear1 = torch.nn.Linear(in_features=in_features, out_features=512, bias=True)\n",
    "        ...\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], x.shape[1] * x.shape[2])\n",
    "        z = self.linear1(x)\n",
    "        z = torch.nn.functional.relu(z)\n",
    "        ...\n",
    "        y = torch.nn.functional.log_softmax(z, dim=1)\n",
    "        \n",
    "        return y\n",
    "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "\n",
    "## Test the train function\n",
    "eta = 0.01\n",
    "n_epochs = 10\n",
    "model = MLP(in_features=x_train.shape[1] * x_train.shape[2], out_features=10)\n",
    "model = model.cuda()\n",
    "train_objective_list, val_objective_list = train(model, eta, n_epochs, train_loader,\n",
    "                                                 torch.tensor(x_val).float(),\n",
    "                                                 torch.tensor(y_val).long())\n",
    "\n",
    "## Plot\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(np.arange(len(train_objective_list)), train_objective_list, label='Train')\n",
    "ax.plot(np.arange(len(val_objective_list)), val_objective_list, label='Validation')\n",
    "ax.set_title(r'$\\eta={' + f'{eta:g}' + r'}$')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Objective');"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Selecting the learning rate\n",
    "\n",
    "✍️ Complete the following code to plot the training figure (of the train and validation objectives over as a function of the number of steps) for the following learning rates: $[10^{-1},3\\cdot10^{-2},10^{-2},3\\cdot10^{-3}]$\n",
    "\n",
    "-Use: n_epochs = 30."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "n_epochs = 30\n",
    "etas_list = (1e-1, 3e-2, 1e-2, 3e-3)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(6, 6))\n",
    "for i_eta, eta in enumerate(etas_list):\n",
    "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "    model = ...\n",
    "    train_objective_list, val_objective_list = train(...\n",
    "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "    \n",
    "    ## Plot\n",
    "    ax = axes.flat[i_eta]\n",
    "    ax.plot(np.arange(len(train_objective_list)), train_objective_list, label='Train')\n",
    "    ax.plot(np.arange(len(val_objective_list)), val_objective_list, label='Validation')\n",
    "    ax.set_title(r'$\\eta={' + f'{eta:g}' + r'}$')\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Objective')\n",
    "axes[1,1].legend()\n",
    "fig.tight_layout()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "✍️ Use $\\eta=0.003$ and fill in the code below to train the model for 2000 epochs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "eta = ...\n",
    "n_epochs = ...\n",
    "model = ...\n",
    "train_objective_list, val_objective_list = train(...\n",
    "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "\n",
    "## Plot\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(np.arange(len(train_objective_list)), train_objective_list, label='Train')\n",
    "ax.plot(np.arange(len(val_objective_list)), val_objective_list, label='Validation')\n",
    "ax.set_title(r'$\\eta={' + f'{eta:g}' + r'}$')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Objective')\n",
    "ax.set_ylim(0, 0.2);"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Early Stopping\n",
    "\n",
    "We would like to stop the training when the validation objective reaches its minimum.\n",
    "\n",
    "✍️ Fill in the code below to rerun the training for the number of epoch which brings the validation objective to its minimum"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "optimal_number_of_steps = ...\n",
    "...\n",
    "train_objective_list, val_objective_list = ...\n",
    "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "\n",
    "## Plot\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(np.arange(len(train_objective_list)), train_objective_list, label='Train')\n",
    "ax.plot(np.arange(len(val_objective_list)), val_objective_list, label='Validation')\n",
    "ax.set_title(r'$\\eta={' + f'{eta:g}' + r'}$')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Objective')\n",
    "ax.set_ylim(0, 0.5);"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluating the Model on the Test Set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    test_score_list = []\n",
    "    for x, y in test_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "        y_hat = model(x)\n",
    "        test_score = (torch.argmax(y_hat, dim=1) != y).sum()\n",
    "\n",
    "        test_score_list.append(test_score.item())\n",
    "\n",
    "test_score = np.sum(test_score_list) / len(test_set)\n",
    "\n",
    "print(f'The test score is: {test_score:.3}')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CNN\n",
    "\n",
    "We will now also implement a CNN network.\n",
    "\n",
    "✍️ Complete the code below to define a CNN which is composed of the following layers:\n",
    "\n",
    "- **conv1**: A convolutional layer with a 3x3 kernel, 1 input channels, 64 output channels, a padding of 1 on each edge + ReLU.\n",
    "- **conv2**: A convolutional layer with a 3x3 kernel, 64 input, 64 output, padding of 1, **stride of 2** + ReLU.\n",
    "- **fc3**: A fully connected of (64\\*4\\*4)x256 + ReLU.\n",
    "- **fc4**: A fully connected of 256x256.\n",
    "- A softmax function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "class CNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features):   \n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        ## Defining the convolutional and fully connected layers with their parameters\n",
    "        ## ===========================================================================\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "        ...\n",
    "        self.fc5 = torch.nn.Linear(in_features=256 * 2 * 2, out_features=256, bias=True)\n",
    "        ...\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x[:, None, :, :]\n",
    "        \n",
    "        z = self.conv1(x)\n",
    "        z = torch.nn.functional.relu(z)\n",
    "        ...\n",
    "\n",
    "        z = z.view(z.shape[0], -1)\n",
    "        \n",
    "        z = self.fc5(z)\n",
    "        ...\n",
    "        y = torch.nn.functional.log_softmax(z, dim=1)\n",
    "        \n",
    "        return y\n",
    "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "\n",
    "## Test the train function\n",
    "eta = 0.01\n",
    "n_epochs = 10\n",
    "model = CNN(in_features=x_train.shape[1] * x_train.shape[2], out_features=10)\n",
    "model = model.cuda()\n",
    "train_objective_list, val_objective_list = train(model, eta, n_epochs, train_loader,\n",
    "                                                 torch.tensor(x_val).float(),\n",
    "                                                 torch.tensor(y_val).long())\n",
    "\n",
    "## Plot\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(np.arange(len(train_objective_list)), train_objective_list, label='Train')\n",
    "ax.plot(np.arange(len(val_objective_list)), val_objective_list, label='Validation')\n",
    "ax.set_title(r'$\\eta={' + f'{eta:g}' + r'}$')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Objective');"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "n_epochs = 20\n",
    "etas_list = (1e-1, 3e-2, 1e-2, 3e-3)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(6, 6))\n",
    "for i_eta, eta in enumerate(etas_list):\n",
    "    model = CNN(in_features=x_train.shape[1] * x_train.shape[2], out_features=10)\n",
    "    model = model.cuda()\n",
    "    train_objective_list, val_objective_list = train(model, eta, n_epochs, train_loader,\n",
    "                                                     torch.tensor(x_val).float(),\n",
    "                                                     torch.tensor(y_val).long())\n",
    "    \n",
    "    ## Plot\n",
    "    ax = axes.flat[i_eta]\n",
    "    ax.plot(np.arange(len(train_objective_list)), train_objective_list, label='Train')\n",
    "    ax.plot(np.arange(len(val_objective_list)), val_objective_list, label='Validation')\n",
    "    ax.set_title(r'$\\eta={' + f'{eta:g}' + r'}$')\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Objective')\n",
    "axes[1,1].legend()\n",
    "fig.tight_layout()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "✍️ Fill in the following cells to:\n",
    "\n",
    "1. Use $\\eta=0.001$ and fill in the code below to train the model for 2000 epochs.\n",
    "2. Retrain the model using early stopping.\n",
    "3. Calculate the misclassification on the test set.\n",
    "\n",
    "Copy the code for plotting the objectives from the cells above."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "eta = 0.003\n",
    "n_epochs = 1000\n",
    "...\n",
    "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "optimal_number_of_steps = ...\n",
    "...\n",
    "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "...\n",
    "test_score = ...\n",
    "print(f'The test score is: {test_score:.3}')\n",
    "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this case of a small image and a small dataset the CNN is not expected to perform better then the MLP."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reminder: Submission\n",
    "\n",
    "To submit your code, download it as a **ipynb** file from Colab, and upload it to the course's website. You can download this code by selecting **Download .ipynb** from the **file** menu."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
