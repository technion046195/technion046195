{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP_LJ0eAhXQs"
      },
      "source": [
        "# Assignment 1 - Linear Least Squares (LLS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2g8u22xhXQ4"
      },
      "source": [
        "## Intro\n",
        "\n",
        "The hands-on assignments (תרגילים רטובים) in this course will be given in python using a free online platform provided by google called Colab. If you need an introduction to this platform please go over assignment 0.\n",
        "\n",
        "### Making your own copy of the notebook\n",
        "\n",
        "Start by making your own copy of this notebook by pressing the \"Copy to drive\" button at the top area of the this page.  Make sure that the logo at the top of the page changes to Google's drive icon: ![](https://ssl.gstatic.com/images/branding/product/1x/drive_2020q4_32dp.png)\n",
        "\n",
        "### Expending all cells\n",
        "\n",
        "When you open a new notebook some sections might be hidden (collapsed), showing only the title of the section with a \"↳ # cells hidden\" note beneath it. Before you start, make show you expand all the sections. You can do so by pressing **Ctrl+[** or by clicking on all the \"↳ # cells hidden\" lines.\n",
        "\n",
        "### ✍️ Your assignments\n",
        "\n",
        "In these assignments, you will have to follow the instruction in the notebooks and fill in the missing code blocks according to the instructions. The cells containing the instructions will be marked with ✍️. The line of code which you will be expected to fill in will be surrounded by the two following comments:\n",
        "\n",
        "\n",
        "``` python\n",
        "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "\n",
        "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "```\n",
        "\n",
        "After filling in all the missing code you will have to download the note book and submit it through the Moodle page. The instructions for downloading the notebook appear at the end of this notebook.\n",
        "\n",
        "### Your IDs\n",
        "\n",
        "✍️ Fill in your IDs in the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERRSL5tuhXQ8",
        "outputId": "2c2420aa-e977-4d46-d4f1-0dd05f704069"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello 012345678 & 012345678\n"
          ]
        }
      ],
      "source": [
        "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "## Fill in your IDs (as a string)\n",
        "student1_id = '...'\n",
        "student2_id = '...'\n",
        "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "\n",
        "print('Hello ' + student1_id + ' & ' + student2_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtxBY-T6hXRF"
      },
      "source": [
        "### Importing Packages\n",
        "\n",
        "We will start by importing the packages which we will be using in this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "rkI41JQ6hXRG"
      },
      "outputs": [],
      "source": [
        "import numpy as np  # Numerical package (mainly multi-dimensional arrays and linear algebra)\n",
        "import pandas as pd  # A package for working with data frames\n",
        "import matplotlib.pyplot as plt  # A plotting package\n",
        "\n",
        "## Setup matplotlib to output figures into the notebook\n",
        "%matplotlib inline\n",
        "\n",
        "## Set some default values of the the matplotlib plots\n",
        "plt.rcParams['figure.figsize'] = (6.0, 6.0)  # Set default plot's sizes\n",
        "plt.rcParams['figure.dpi'] = 120  # Set default plot's dpi (increase fonts' size)\n",
        "plt.rcParams['axes.grid'] = True  # Show grid by default in figures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INrEFQ4jhXRI"
      },
      "source": [
        "## The pandas package\n",
        "\n",
        "A great and very popular package for working with dataset in Python is the [pandas package](https://pandas.pydata.org/), which we have imported in the above code cell.\n",
        "\n",
        "The main type of object in pandas is the *DataFrame*. The data frame is basically 2D Mumpy arrays with extended capabilities such as:\n",
        "\n",
        "- The ability to give names to columns \\ rows.\n",
        "- Advance manipulation such as: grouping, querying etc. handing missing data.\n",
        "- Predefined plotting function.\n",
        "- Nice integration with notebooks (such as this one) viewing the array as a nice table\n",
        "\n",
        "In this course we will mostly be using this package to load our dataset and then extract the relevant data form them as a Numpy array.\n",
        "\n",
        "If you are interested in learning more about pandas you can look at the [10 minutes to pandas](https://pandas.pydata.org/docs/user_guide/10min.html) page or go through the [full official tutorial](https://pandas.pydata.org/docs/getting_started/index.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFGj0dQlhXRK"
      },
      "source": [
        "As a very short introduction to working with pandas DataFrames we will give a few quick examples.\n",
        "\n",
        "(**Reminder**: when running a cell, the output of the last line in the cell is presented below the cell as the output of the cell)\n",
        "\n",
        "1. The following code wraps a 5 by 5 random array (with integers between 0 and 10) with a DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82g4UqJrhXRL"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(np.random.randint(0, 10, (5, 5)))\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF5KBtNbhXRN"
      },
      "source": [
        "2. We can also give each column a name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfTjtmjjhXRO"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(np.random.randint(0, 10, (5, 5)), columns=['field 1', 'field 2', 'field 3', 'field 4', 'field 5'])\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsPLQXeJhXRQ"
      },
      "source": [
        "3. To extract a specific columns we can address it by it's name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhQWvsNchXRR"
      },
      "outputs": [],
      "source": [
        "print('The column of \"field 3\" is:')\n",
        "df['field 3']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBaF4L0KhXRS"
      },
      "source": [
        "4. To convert a column / DataFrame to a Numpy array we can use the \".value\" attribute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVLBsL3rhXRT"
      },
      "outputs": [],
      "source": [
        "df['field 3'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84dQ-ZW9hXRU"
      },
      "source": [
        "## Seoul Bike Sharing Demand Dataset\n",
        "\n",
        "In this assignment we will be looking at the data from an automatic bike rental service (similar to \"תל-אופן\") which operates in the city of Seoul. The dataset which we will be working with has been collected for every hour of the day over a period of one year. For each hour the dataset contains the number of bikes rented along with a few weather indicator such as the temperature, the wind speed and the amount of rain.\n",
        "\n",
        "In this assignment we will try to predict the number of the bikes which are going to be rented based on the day of the week, the hour of the data and the weather. We shall use RMSE cost as our cost function.\n",
        "\n",
        "This dataset has been made available by the [UCI's (University of California, Irvine) Center for Machine Learning and Intelligent Systems](http://archive.ics.uci.edu/ml/index.php). The project's official site along with the dataset can be found [here](http://archive.ics.uci.edu/ml/datasets/Seoul+Bike+Sharing+Demand).\n",
        "\n",
        "We will work with a slightly modified version of this dataset, which can be found [here](https://technion046195.github.io/semester_2019_spring/datasets/bike_demand.csv)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoP2e4d9hXRV"
      },
      "source": [
        "### Loading the dataset\n",
        "\n",
        "We shall start by loading the dataset. The dataset is stored as an [CSV file](https://en.wikipedia.org/wiki/Comma-separated_values). The \"read_csv\" function in the pandas package reads a CSV file into a DataFrame (even for a remote location on the web)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "oaJM53tIhXRW"
      },
      "outputs": [],
      "source": [
        "dataset_location = 'https://technion046195.netlify.app/datasets/bike_demand.csv'\n",
        "\n",
        "## Loading the data\n",
        "dataset = pd.read_csv(dataset_location)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD7jqbBIhXRX"
      },
      "source": [
        "Lets look at the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFzL5kwJhXRY"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP_nRLLxhXRY"
      },
      "source": [
        "The column of \"Rented Bike Count\" will be our labels ($\\text{y}$) all the rest except for the \"Date\" will be our measurements ($\\mathbf{x}$).\n",
        "\n",
        "**Think**: Why would learning to make predictions based on the date be a bad thing?\n",
        "\n",
        "✍️ Complete the code in order to create a list named \"x_field\", which contain the name of the fields which we would like to use as the measurements:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "d3lavhWbhXRZ"
      },
      "outputs": [],
      "source": [
        "y_field = 'Rented Bike Count'\n",
        "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "## Fill in the name of the relevant fields.\n",
        "x_fields = ['Hour', 'Temperature(°C)', ...\n",
        "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTN2H9_ThXRa"
      },
      "source": [
        "### Plotting the $\\text{y}$ vs. each one of the fields\n",
        "\n",
        "In order to get a fell of the data (values ranges, distribution across the range, etc.) let's create a plot for the values in each field with the matching values of $\\text{y}$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "KUhPwFvRhXRb"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(3, 3, figsize=(9, 9))  # Create a figure with 3 over 3 axes\n",
        "axes = axes.flatten()  # Convert the object storing the axes for a 3 by 3 array into a 1D array of lenth 9.\n",
        "\n",
        "for i_field, field in enumerate(x_fields):\n",
        "    ax = axes[i_field]\n",
        "    ax.plot(dataset[field].values, dataset['Rented Bike Count'].values, '.', markersize=1)\n",
        "    ax.set_xlabel(field)\n",
        "    ax.set_ylabel('Rented Bike Count')\n",
        "fig.tight_layout()    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPGBzjhKhXRc"
      },
      "source": [
        "## Train-test split\n",
        "\n",
        "Before we start building our model we shall first divide the dataset into 80% train and 20% train.\n",
        "\n",
        "**Note**: we will use the name train_full to indicate the train set before splitting it into train and validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "vrpB2qJohXRd"
      },
      "outputs": [],
      "source": [
        "n_samples = len(dataset)  # The total number of samples in the dataset\n",
        "\n",
        "## Generate a random generator with a fixed seed\n",
        "rand_gen = np.random.RandomState(0)\n",
        "\n",
        "## Generating a shuffled vector of indices\n",
        "indices = np.arange(n_samples)\n",
        "rand_gen.shuffle(indices)\n",
        "\n",
        "## Split the indices into 80% train (full) / 20% test\n",
        "n_samples_train_full = int(n_samples * 0.8)\n",
        "train_full_indices = indices[:n_samples_train_full]\n",
        "test_indices = indices[n_samples_train_full:]\n",
        "\n",
        "## Extract the sub datasets from the full dataset using the calculated indices\n",
        "train_full_set = dataset.iloc[train_full_indices]\n",
        "test_set = dataset.iloc[test_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf70YOKlhXRd"
      },
      "source": [
        "## Train - validation split\n",
        "\n",
        "Since we plan to test different configuration of building our model we would like to further split the train set into 75% train and 25% validation set. (which will result in a 60%-20%-20% over all split).\n",
        "\n",
        "✍️ Complete the following code to create a train-validation split similar to the train-test split above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "RbuexPIzhXRe"
      },
      "outputs": [],
      "source": [
        "## Generate a random generator with a fixed (different) seed\n",
        "rand_gen = np.random.RandomState(1)\n",
        "\n",
        "## Generating a shuffled vector of indices\n",
        "indices = train_full_indices.copy()\n",
        "rand_gen.shuffle(indices)\n",
        "\n",
        "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "## Split the indices of the train (full) dataset into 75% train / 25% validation\n",
        "n_samples_train = ...\n",
        "train_indices = ...\n",
        "val_indices = ...\n",
        "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "\n",
        "## Extract the sub datasets from the full dataset using the calculated indices\n",
        "train_set = dataset.iloc[train_indices]\n",
        "val_set = dataset.iloc[val_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT0qenVUhXRf"
      },
      "source": [
        "## Training an LLS model based on a single field\n",
        "\n",
        "As a warm up, we shall start with building a linear predictor using only the temperature field for the dataset. In other words we would like to use the following parametric model:\n",
        "\n",
        "$$\n",
        "\\hat{y}=h_{\\boldsymbol{\\theta}}(\\boldsymbol{x})=\\theta_1+\\theta_2 x_{\\text{temperature}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXkY_LGqhXRg"
      },
      "source": [
        "### Building the $X$ matrix and the $\\boldsymbol{y}$ vector\n",
        "\n",
        "Since we would like our model to also have a bias term, we would like to construct an $X$ matrix with a column containing the temperature field and a column containing ones.\n",
        "\n",
        "The following code constructs the $X$ matrix and the $\\boldsymbol{y}$ vector:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "yCgJm7tYhXRg"
      },
      "outputs": [],
      "source": [
        "## Create a list of all the features we would like X to contain\n",
        "features = []\n",
        "features.append(np.ones(len(dataset)))\n",
        "features.append(dataset['Temperature(°C)'].values)\n",
        "\n",
        "## Stack all the features side by side to create X\n",
        "x_train = np.stack(features, axis=1)\n",
        "\n",
        "## Extract the y vector form the dataset\n",
        "y_train = train_set[y_field].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAlk0Hb4hXRh"
      },
      "source": [
        "Printing first 10 samples in $X$ and $\\boldsymbol{y}$: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBZ5tIeQhXRh"
      },
      "outputs": [],
      "source": [
        "print('The X matrix:')\n",
        "print(x_train[:10])\n",
        "print('\\nThe y vector:')\n",
        "print(y_train[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHI3yqJ4hXRi"
      },
      "source": [
        "#### Putting it in a function\n",
        "\n",
        "Here we have constructed $X$ and $\\boldsymbol{y}$ for the train set. Since we would later like to repeat the process for the validation and the test set as well it would be better if we would create a function which receives a dataset (a DataFrame) and extracts $X$ and $\\boldsymbol{y}$.\n",
        "\n",
        "✍️ Complete the following code to create this function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "i73e1jOVhXRj"
      },
      "outputs": [],
      "source": [
        "def extract_x_y(dataset):\n",
        "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "    ## make sure the you use dataset here and not train_set\n",
        "    features = []\n",
        "    ...\n",
        "    x = ...\n",
        "    y = ...\n",
        "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "    return x, y\n",
        "\n",
        "x_train, y_train = extract_x_y(train_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CU-8u0shXRk"
      },
      "source": [
        "### Calculating the model's parameters\n",
        "\n",
        "✍️ Use *x_train* and *y_train* to calculate the optimal parameters $\\boldsymbol{\\theta}$ as we learned in class.\n",
        "\n",
        "Useful Numpy functions:\n",
        "\n",
        "- *A.T* calculates the transpose of A.\n",
        "-  *A@B* performs a matrix multiplication between A and B.\n",
        "- np.linalg.inv(A) calculates the inverse matrix of A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsxQHAkXhXRl"
      },
      "outputs": [],
      "source": [
        "## Calcualting theta\n",
        "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "theta = ...\n",
        "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "print(theta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOJnbzbohXRm"
      },
      "source": [
        "### Using a more numerical efficient calculation\n",
        "\n",
        "The equation we have showed in class for calculating $\\boldsymbol{\\theta}$ from $X$ and $\\boldsymbol{y}$ is mathematically correct but it is not the most efficient and numerically stable way to to find the optimal $\\boldsymbol{\\theta}$ minimizing $\\lVert X\\boldsymbol{\\theta}-\\boldsymbol{y}\\rVert_2^2$. In general when performing numerical calculation we would prefer to avoid inverting matrices.\n",
        "\n",
        "The Numpy function *np.linalg.lstsq* solve the least squares problem in a much more efficient and stable way. Therefore we would prefer calculating $\\boldsymbol{\\theta}$ in the following way:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OylMxArbhXRm"
      },
      "outputs": [],
      "source": [
        "theta, _, _, _ = np.linalg.lstsq(x_train, y_train, rcond=None)\n",
        "print(theta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRErSERnhXRn"
      },
      "source": [
        "Make sure you got the same results in your calculation as the *np.linalg.lstsq* function.\n",
        "\n",
        "- The *rcond=None* argument was added to suppress some error which is related to some recent changes which were made in the Numpy package.\n",
        "- The *np.linalg.lstsq* function return 4 outputs (the other 3 outputs are related to the quality of the solution). The \", _, _, _\" phrase which appears in the above equation tells Python to expect 4 outputs but to ignore the last 3.\n",
        "\n",
        "The full documentation of the *np.linalg.lstsq* function can be found [here](https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5du7T0UDhXRo"
      },
      "source": [
        "### Making predictions\n",
        "\n",
        "✍️ Use *theta* and *x_train* to calculate the predicted number of rents, $\\hat{y}$, on the train set.\n",
        "\n",
        "- Use the *@* operator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rc1hfdC-hXRo"
      },
      "outputs": [],
      "source": [
        "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "y_hat_train = ...\n",
        "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "\n",
        "## Printing the predictions and the true values of the 20 first samples\n",
        "print(f'Predictions: {y_hat_train[:10].astype(int)}')\n",
        "print(f'Predictions: {y_train[:10]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cy04Q_Y9hXRp"
      },
      "source": [
        "### Building a class\n",
        "\n",
        "To make building the model and making prediction simpler we shall build a class which will hold the learned model and will have two methods:\n",
        "\n",
        "- **fit**: which will receive $X$ and $\\boldsymbol{y}$ and use them to lean the optimal parameters.\n",
        "- **predict** which will receive $X$ and produce $\\hat{y}$.\n",
        "\n",
        "✍️ Complete the missing line in the following code based on the prediction function you wrote above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "1VJGXxN7hXRq"
      },
      "outputs": [],
      "source": [
        "class LLS:  # Defining a new class named LLS\n",
        "    def __init__(self):  # This function is executed when the object is created\n",
        "        ## Defining and initizaling the classes inner variables\n",
        "        self.theta = None  # Theta will hold the learned model parameters.\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        self.theta, _, _, _ = np.linalg.lstsq(x, y, rcond=None)\n",
        "    \n",
        "    def predict(self, x):\n",
        "        ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "        ## Make sure you use x here and not x_train\n",
        "        y_hat = ...\n",
        "        ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "        return y_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiAhwSs-hXRr"
      },
      "source": [
        "Let's use our new class for training a model and making predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsBKqJMuhXRr"
      },
      "outputs": [],
      "source": [
        "model = LLS()\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "y_hat_train = model.predict(x_train)\n",
        "\n",
        "## Printing the predictions and the true values of the 20 first samples\n",
        "print(y_hat_train[:10].astype(int))\n",
        "print(y_train[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85JXdhUJhXRs"
      },
      "source": [
        "Make sure you got the same results as before"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgVC0Yf3hXRs"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "✍️ Complete the function bellow so it will evaluate the RMSE score for a given model (an LLS object) and a given pair of $X$ and $\\boldsymbol{y}$:\n",
        "\n",
        "- **Reminder**: the RMSE score is given by $\\sqrt{\\tfrac{1}{N}\\sum_i (\\hat{y}_i-y_i)^2}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "nn5wQUc1hXRt"
      },
      "outputs": [],
      "source": [
        "def calc_rmse(model, x, y):\n",
        "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "    y_hat = model.predict(...\n",
        "    rmse = ...\n",
        "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "    return rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmdNnyzVhXRt"
      },
      "source": [
        "Lets use this function to calculate the train and the validation scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_Ns8LhghXRu"
      },
      "outputs": [],
      "source": [
        "## Calculate the model's rmse on the train set\n",
        "print(f'The train score is: {calc_rmse(model, x_train, y_train):.2f}')\n",
        "\n",
        "## Extract x_val and y_val form val_set\n",
        "x_val, y_val = extract_x_y(val_set)\n",
        "\n",
        "## Calculate the model's rmse on the validation set\n",
        "print(f'The validation score is: {calc_rmse(model, x_val, y_val):.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DILxNFS1hXRv"
      },
      "source": [
        "### Using the scikit-learn package\n",
        "\n",
        "The [scikit-learn package](https://scikit-learn.org/stable/) (which appears as sklearn in Python) is a popular Python package for applying machine-learning algorithms. The package contains almost all the algorithms which we will cover in this course.\n",
        "\n",
        "We will use this package in many places in these assignments.\n",
        "\n",
        "The scikit-learn package has an implementation of a [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) class which is very similar to the one we wrote. Let's try using this implementation and compare our results.\n",
        "\n",
        "**notes**:\n",
        "\n",
        "- We will usually only import from scikit-learn the specific class \\ function we need rather then the full package. This is done using the following syntax: \"from sklearn.linear_model import LinearRegression\"\n",
        "- The LinearRegression class added the the bias term by default, since we have already added it ourselves we will tell the class not to add it by using the \"fit_intercept=False\" argument when creating the model.\n",
        "- We will import from scikit-learn two more classes we will need to use in the following assigments, 'make_pipeline' and 'StandarScaler'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGAsfoQEhXRv"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "model = LinearRegression(fit_intercept=False)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "print(f'The train score is: {calc_rmse(model, x_train, y_train):.2f}')\n",
        "print(f'The validation score is: {calc_rmse(model, x_val, y_val):.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndQgc7AehXRw"
      },
      "source": [
        "Make sure you got the same results as before"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1DiIvmbhXRw"
      },
      "source": [
        "## Using all features\n",
        "\n",
        "Up until now we have only used the temperature files from the dataset. We would now like to expend our model to include all features:\n",
        "\n",
        "$$\n",
        "\\hat{y}=h_{\\boldsymbol{\\theta}}(\\boldsymbol{x})\n",
        "=\\theta_1\n",
        "+\\theta_2 x_{\\text{hour}}\n",
        "+\\theta_3 x_{\\text{temperature}}\n",
        "+\\theta_4 x_{\\text{humidity}}\n",
        "+\\dots\n",
        "$$\n",
        "\n",
        "✍️ Update the \"extract_x_y\" function so that it will produce new $X$ which has a field for each column.\n",
        "\n",
        "- Do not add the ones column (the bias column). We will let the LinearRegression add it by itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "zeN31ogbhXRx"
      },
      "outputs": [],
      "source": [
        "def extract_x_y(dataset):\n",
        "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "    features = []\n",
        "    for field in x_fields:\n",
        "        features.append(...\n",
        "    \n",
        "    x = ...\n",
        "    y = ...\n",
        "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTYOKHC5hXRy"
      },
      "source": [
        "Let's train and evaluate the linear model using the new features.\n",
        "\n",
        "- We have removed the \"fit_intercept=False\" argument so that the LinearRegression will add a bias term.\n",
        "- We would like to normalize each column of $X$, making the learning process of the parameters more stable (ask yourself why?). In order to do it we will use 'make_pipeline' and 'StandardSvaler' which we alredy imported."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzdiphuvhXRy"
      },
      "outputs": [],
      "source": [
        "x_train, y_train = extract_x_y(train_set)\n",
        "x_val, y_val = extract_x_y(val_set)\n",
        "\n",
        "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "print(f'The train score is: {calc_rmse(model, x_train, y_train):.2f}')\n",
        "print(f'The validation score is: {calc_rmse(model, x_val, y_val):.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipUvpxPShXRz"
      },
      "source": [
        "**Check**: Have the result improve? Both of them?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eu1Yg-z7hXR0"
      },
      "source": [
        "## Adding second order features\n",
        "\n",
        "Now let's try to use a parametric model which is a second order polynomial of the fields in the dataset.\n",
        "\n",
        "$$\n",
        "\\hat{y}=h_{\\boldsymbol{\\theta}}(\\boldsymbol{x})\n",
        "=\\theta_1\n",
        "+\\theta_2 x_{\\text{hour}}\n",
        "+\\theta_3 x_{\\text{temperature}}\n",
        "+\\theta_4 x_{\\text{hour}}^2\n",
        "+\\theta_5 x_{\\text{temperature}}^2\n",
        "+\\theta_6 x_{\\text{hour}}x_{\\text{temperature}}\n",
        "+\\dots\n",
        "$$\n",
        "\n",
        "✍️ Update the \"extract_x_y\" function so that $X$ will include all the term which should appear in a second order polynomial of the features in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHK8eZdohXR1"
      },
      "outputs": [],
      "source": [
        "def extract_x_y(dataset):\n",
        "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "    features = []\n",
        "    ## Adding first order features\n",
        "    for field in x_fields:\n",
        "        features.append(...\n",
        "    ## Adding second order features\n",
        "    for field1 in x_fields:\n",
        "       for field2 in x_fields:\n",
        "           features.append(...\n",
        "    \n",
        "    x = ...\n",
        "    y = ...\n",
        "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "    return x, y\n",
        "\n",
        "x_train, y_train = extract_x_y(train_set)\n",
        "x_val, y_val = extract_x_y(val_set)\n",
        "\n",
        "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "print(f'The train score is: {calc_rmse(model, x_train, y_train):.2f}')\n",
        "print(f'The validation score is: {calc_rmse(model, x_val, y_val):.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLW-3B80hXR2"
      },
      "source": [
        "**Check**: Have the result improve? Both of them?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv5_HH5fhXR2"
      },
      "source": [
        "## Adding third order features\n",
        "\n",
        "Now let's try to use a parametric model which is a third order polynomial of the fields in the dataset.\n",
        "\n",
        "✍️ Update the \"extract_x_y\" function so that $X$ will include all the term which should appear in a third order polynomial of the features in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZmk_GzihXR3"
      },
      "outputs": [],
      "source": [
        "def extract_x_y(dataset):\n",
        "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "    features = []\n",
        "    ## Adding first order features\n",
        "    for field in x_fields:\n",
        "        features.append(...\n",
        "    ## Adding second order features\n",
        "    for field1 in x_fields:\n",
        "       for field2 in x_fields:\n",
        "           features.append(...\n",
        "    ## Adding third order features\n",
        "    for field1 in x_fields:\n",
        "        for field2 in x_fields:\n",
        "            for field3 in x_fields:\n",
        "                features.append(...\n",
        "    \n",
        "    x = ...\n",
        "    y = ...\n",
        "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "    return x, y\n",
        "\n",
        "x_train, y_train = extract_x_y(train_set)\n",
        "x_val, y_val = extract_x_y(val_set)\n",
        "\n",
        "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "print(f'The train score is: {calc_rmse(model, x_train, y_train):.2f}')\n",
        "print(f'The validation score is: {calc_rmse(model, x_val, y_val):.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1Z73OVuhXR4"
      },
      "source": [
        "**Check**: If you implemented every thing correctly the results here should not have improved. What happened? (you do not have to write down the answer, just make sure you know the answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OgUECw5hXR4"
      },
      "source": [
        "## Ridge - $l_2$ regularization\n",
        "\n",
        "Let's try adding an $l_2$ regularization. We can do so by simply replacing the *LinearRegression* class we have been using with the scikit-learn package of [*Ridge*](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html).\n",
        "\n",
        "The *Ridge* class works just the same as the *LinearRegression* class except that we need to give it the an argument *alpha* which is the regularization coefficient. In our course we use $\\lambda$ for this coefficient.\n",
        "\n",
        "Just as a quick check, let's try training a model using Ridge with $\\lambda=0$.\n",
        "\n",
        "**Think**: what results do you expect to get?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26dFQP7ihXR5"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "model = make_pipeline(StandardScaler(with_mean=False), Ridge(alpha=0))\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "print(f'The train score is: {calc_rmse(model, x_train, y_train):.2f}')\n",
        "print(f'The validation score is: {calc_rmse(model, x_val, y_val):.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66SUiC0ghXR6"
      },
      "source": [
        "## Selecting $\\lambda$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbuUDtDGhXR6"
      },
      "source": [
        "To select $\\lambda$ we will simply try a set of values and check how well they perform on the validation set.\n",
        "\n",
        "✍️ Complete the following code to calculate the train and validation scores for the given list of values if $\\lambda$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "U0PONjHehXR7"
      },
      "outputs": [],
      "source": [
        "## The list of lambdas to test\n",
        "lambdas_list = [3, 1, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001, 0.0003, 0.0001]\n",
        "\n",
        "## We will use these to dictionaries to store the results for each lambda\n",
        "train_scores = {}\n",
        "val_scores = {}\n",
        "\n",
        "for lambda_value in lambdas_list:\n",
        "    model = make_pipeline(StandardScaler(with_mean=False), Ridge(alpha=lambda_value))\n",
        "\n",
        "    model.fit(x_train, y_train)\n",
        "    \n",
        "    train_scores[lambda_value] = calc_rmse(model, x_train, y_train)\n",
        "    val_scores[lambda_value] = calc_rmse(model, x_val, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaoU51HhhXR7"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.set_xscale('log')\n",
        "ax.plot(list(train_scores.keys()), list(train_scores.values()), label='Train')\n",
        "ax.plot(list(val_scores.keys()), list(val_scores.values()), label='Validation')\n",
        "ax.set_ylabel('RMSE')\n",
        "ax.set_xlabel('$\\lambda$')\n",
        "ax.set_title('Score vs. $\\lambda$')\n",
        "ax.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgNIRSPKhXR8"
      },
      "source": [
        "The following code selects the $\\lambda$ with the lowest validation score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lu9mV19-hXR9"
      },
      "outputs": [],
      "source": [
        "best_lambda = min(val_scores.keys(), key=val_scores.get)\n",
        "print(f'The best lambda is: {best_lambda}, with a validation score of {val_scores[best_lambda]:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LydCa8TnhXR9"
      },
      "source": [
        "## LASSO - $l_1$ regularization\n",
        "\n",
        "We can also test $l_1$ regularization using the *Lasso* class in scikit-learn.\n",
        "\n",
        "As we learned in class LASSO has no close form solution and we must use numerical methods to solve it. Scikit-learn uses an iterative algorithm called [coordinate descent](https://en.wikipedia.org/wiki/Coordinate_descent) to find the solution. The smaller $\\lambda$ is the more steps it takes the algorithm to converge.\n",
        "\n",
        "In general we would also want to test different values of $\\lambda$ for LASSO as well, but since LASSO takes slightly longer to learn, we shall save you the time of running the algorithm multiple times, and we shall only test $\\lambda=0.01$.\n",
        "\n",
        "In order to make the algorithm converge we will need to increase the maximal number of iteration it can use. We will do so using the *max_iter* argument:\n",
        "\n",
        "(Running his cell can take about **20 min**)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gD_f1HrkhXR-"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "model = make_pipeline(StandardScaler(with_mean=False), Lasso(alpha=0.01, max_iter=100000))\n",
        "model.fit(x_train, y_train)\n",
        "    \n",
        "print(f'The train score is: {calc_rmse(model, x_train, y_train):.2f}')\n",
        "print(f'The validation score is: {calc_rmse(model, x_val, y_val):.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgdduGV1hXR-"
      },
      "source": [
        "## Retraining using the full train set\n",
        "\n",
        "✍️ In order to produce the final model select the method which gave the best score on the validation set and train a model using this method and the train_full_set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJs6YlT1hXR_"
      },
      "outputs": [],
      "source": [
        "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "x_train_full, y_train_full = ...\n",
        "\n",
        "model = ...\n",
        "model.fit(....\n",
        "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoIlLT9ahXSA"
      },
      "source": [
        "## Final evaluation\n",
        "\n",
        "Evaluate the model using the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MeCTNmrhXSA"
      },
      "outputs": [],
      "source": [
        "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "...\n",
        "print(f'The validation score is: {...\n",
        "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sx9zOPThXSB"
      },
      "source": [
        "## Submission\n",
        "\n",
        "To submit your code download it as a **ipynb** file from Colab, and upload it to the course's website (Moodle). You can download this code by selecting **Download .ipynb** from the **file** menu."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}