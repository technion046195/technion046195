---
type: lecture
index: 20
template: page
make_docx: true
print_pdf: true
---

<div dir="rtl" class="site-style">

# הרצאה 5 - יסודות בלמידה חישובית

<div dir="ltr">
<a href="/assets/lecture05.pdf" class="link-button" target="_blank">PDF</a>
</div>

## מה נלמד היום

<div class="imgbox" style="max-width:900px">

![](./assets/course_diagram.png)

</div>
בפרק זה נציג מעט מהתיאוריה הכמותית הקיימת בנושא למידה והכללה. המטרה הבסיסית של תיאוריה זו היא תיאור כמותי של בעיית הלמידה, אפיון הביצועים האפשריים עבור בעיית למידה נתונה, וחקר כמותי של השפעת המרכיבים השונים של הבעיה (כגון: סיבוכיות המודל, אופן בחירת הדגימות, מספר הדגימות, וכו') על הביצועים המתקבלים. 

תיאוריה זו היא בעיקרה **בעלת אופי סטטיסטי**, כלומר מסתמכת על **כלים הסתברותיים**. 

אנו נסתפק בהצגת מספר תוצאות ומושגים  יסודיים, וזאת עבור **בעיית הסיווג הבינארי בלבד**.

## מודל הלמידה הבסיסי

נזכור כי בבעיית הלמידה המודרכת אנו נדרשים "ללמוד" פונקציה $\hat{y}=h(\boldsymbol{x})$ על סמך מדגם $\mathcal{D}=\{\boldsymbol{x}^{(i)}, y^{(i)}\}_{i=0}^N$. 

המודל הבסיסי בו נעסוק כולל את המרכיבים הבאים: 

- **פונקצית החיזוי** - פונקציה $\hat{y}=h(\boldsymbol{x})$ ממרחב הקלט $\mathcal{X}$ למרחב היציאה $\mathcal{Y}$ אותו אנו רוצים ללמוד. נזכיר כי עבור בעיית רגרסיה מתקיים $\mathcal{Y}=\mathbb{R}$ ועבור בעיית הסיווג הבינארי מתקיים $\mathcal{Y}=\{-1, 1\}$. נניח כי התיוג דטרמניסטי.

- **מודל בחירת הדוגמאות** - דוגמאות הקלט נבחרות באופן בלתי תלוי ולפי פילוג הסתברות קבוע (אך לא בהכרח ידוע), כלומר באופן i.i.d. כלומר מתקיים, $\mathbf{x} \sim P_X, i=1,...,N$. הדוגמאות מתויגות באופן מושלם לפי $h_0$, כלומר $y^{(i)}=h_0(x^{(i)})$.

- **מודל פרמטרי** - אוסף $H$ של פונקציות $H: \mathcal{X} \rightarrow \mathcal{Y}$, שמתוכו נבחר את הפנוקציה $\hat{h}$ אשר משערכת את פונקציית המטרה $h$. $H$ תכונה כאן **מחלקת ההשערות**.

**מדד הביצועים** עבור השערה $h \in H$ כלשהי יהיה מהצורה 

$$L(h)=E[\mathcal{l}(h(x),h_0(x))]$$

כאשר:

- $\mathcal{l}(\hat{y},y)$ הינה פונקצית מחיר מתאימה. למשל שגיאת $\mathcal{l}_2$ לבעיית רגרסיה או zero-one loss לבעיית סיווג.

- התוחלת היא על המשתנה המקרי $x$ לפי הפילוג $x \sim P_X$. פילוג זה זהה לפילוג לפיו נבחרו הדוגמאות.

- עבור בעיית הסיווג הבינארי נקבל $L(\hat{h})=P\{\hat{h}(x)\neq h_0(x)\}=P_e(\hat{h})$. כאשר המעבר השני נכון בגלל תוחלת של אינדיקטור. 

**מטרת תהליך הלימוד** היא, אם כן, לבחור פונקציה $\hat{h}\in H$ (כתלות במדגם) אשר מביאה את מדד הביצועים $L(h)$ למינימום.

הבעיה היא כמובן ש-$L(h)$ אינו ניתן לחישוב מתוך מדגם סופי!

### הערות

- חשוב להדגיש כי הדוגמאות $\{x^(i)\}$ נבחרות לפי אותו פילוג $P_X$ המשמש בהגדרת מדד הביצועים. דבר זה יאפשר קבלת חסמלים על קצב ושגיאת הלימוד שאינם תלויים ב-$P_X$.

- המודל הנ"ל מניח קשר דטרמניסטי בין $x$ ל-$y$. ניתן להרחיב את התוצאות הללו למקרה של קשר אקראי, כלומר להחליף את הפונקציה $y=h_0(x)$ בפילוג המותנה $p(y|x)$.

המודל ההסתברותי שהגדרנו מאפשר התייחסות כמותית לשאלות הבאות:

- **דיוק הלמידה** - באיזה דיוק ניתן ללמוד את פונקצית המטרה $h_0(x)$ מתוך $N$ דוגמאות?
- **קצב הלמידה** - כמה דוגמאות נדרשות כדי להשיג דיוק נתון?

## מזעור המחרי האמפירי (Emperical RIsk Minimization)

בהיעדר מידע לגב הפילוג, ניתן להחליף את המזעור של קריטריון הביצועים במזעור של פונקציית המחיר האמפירית, אותה אנחנו יכולים לחשב. 

כלומר, בהינתן המדגם $\mathcal{D}=\{\boldsymbol{x}^{(i)}, y^{(i)}\}_{i=0}^N$ , נבחר את ההשערה $\hat{h}_{\mathcal{D}}$ באופן הבא:

$$\hat{h}_{\mathcal{D}}\in \underset{{h\in H}}{\arg\min} \hat{L}_{\mathcal{D}}(h), \qquad \hat{L}_{\mathcal{D}}(h) = \frac{1}{N}\sum_{i=1}^{N}\mathcal{l}(h(x^{(i)}),h_0(x^{(i)})) $$

לדוגמה: 
 
 - עבור מחיר ריבועי נקבל: $\hat{L}_{\mathcal{D}}(h) = \frac{1}{N}\sum_{i=1}^{N}(h(x^{(i)})-h_0(x^{(i)}))^2 $

     שזאת פונקציית המחיר ששימשה אותנו בהקשר של בעיות רגרסיה.

- עבור בעיות סיווג נקבל: $\hat{L}_{\mathcal{D}}(h) = \frac{1}{N}\sum_{i=1}^{N}I(h(x^{(i)})\neq h_0(x^{(i)}))$

     זהו מספר השגיאות הממוצע של המסווג על סט הלימוד.

נניח מעתה כי  $\hat{h}_N$ היא אכן הפונקציה הנבחרת על ידי אלגוריתם הלמידה שלנו.  בפרט, אנו מניחים כי ניתן למצוא את המינימום הגלובאלי של $\hat{L}_N(h)$ , מבלי להתייחס לקושי החישובי הכרוך בכך.

**הערה:**  למרות שאנו מניחים מזעור של השגיאה האמפירית אין לראות בכך  המלצה לעשות זאת! גישה זו יכולה להוביל להתאמת-יתר חמורה עבור מרחב השערות גדול. 


**שגיאת ההכללה לעומת שגיאת הקירוב**

נסמן - $h^* \in \arg_{h\in H} \min \hat{L}(h)$ - ההשערה האופטימלית שאינה ניתנת לחישוב. 

 קריטריון הביצועים המתקבל עבור  ניתן לרישום באופן הבא:

 $$L(\hat{h}_{\mathcal{D}})=L(h^*)+[L(\hat{h}_{\mathcal{D}})-L(h^*)]$$
 
-	האיבר הראשון הוא שגיאת הקירוב (בדומה למשתנה ההטיה, bias), אשר נובע מכך שאנו מגבילים את הפונקציה הנלמדת לקבוצת ההשערות $H$. הוא אינו תלוי במספר הדגימות.

-	האיבר השני הוא שגיאת השערוך (בדומה למשתנה השונות), ומבטא את השגיאה הנובעת מסופיות המדגם עקב כך שהפונקציה הנבחרת  $\hat{h}_{\mathcal{D}}$ אינה האופטימלית (מתוך  $H$). זאת מכיוון שאנו מבצעים מינימיזציה של המחיר האמפירי   במקום של קריטריון הביצועים. 

-	ככל שמחלקת ההשערות   $H$ עשירה (גדולה) יותר, אנו מצפים כי האיבר הראשון יקטן, והאיבר השני יגדל.

-	עושר המודל  ($H$) צריך להיות כזה המוצא איזון אופטימאלי בין שני איברים אלה. 



## חסמים עבור מחלקת השערות סופית
 נתמקד מעתה בבעית הסיווג הבינארי

 $$\mathcal{l}(\hat{y},y)=I\{\hat{y} \neq y\}, \mathcal{Y}=\{-1,+1\}$$

 מטרצנו למצוא חסמים על קריטריון הביצועים $L(\hat{h}_{\mathcal{D}})$, כאשר $\hat{h}_{\mathcal{D}}$ היא הפונקציה (ההשערה) המביאה למינימום את המחיר האמפירי $\hat{L}_{\mathcal{D}}(h)$.

 נשים לב כי במקרה הבינארי המחיר האמפרי איננו אלא השגיאה האמפירית (למה?). 

 ראשית נעסוק במקרה בו $h_0\in H$, כלומר במקרה בו פונקציה המטרה $h_0$ כלולה בתוך קבוצת ההשערות $H$, כלומר $L^*_{H}=\underset{h \in H}{\min}L(h)=0$. 

 **משפט 1** נניח כי $|H| < \infty$ וכן $h_0 \in H$, כלומר $L^*=0$. אזי, השערה $\hat{h}_{\mathcal{D}}$ הממזערת את השגיאה האמפירית מקיימת לכל $\varepsilon > 0$

 $$P(L(\hat{h}_{\mathcal{D}})>\varepsilon) < |H|e^{-\varepsilon n}$$

ניתן להגדיר את המשפט גם בצורה שקולה באמצעות "רווח סמך" (confidence interval). 

רווח סמך הוא מושג מסטטסיטיקה המתאר, עבור פרמטר לא ידוע כלשהו, קטע שמחושב מתוך תוצאות המדגם, כך   שהסיכוי שהקטע שנקבל יכלול את 
הפרמטר הוא קבוע, הקרוי רמת הסמך של הקטע. המשלים לרמת הסמך קרוי רמת המובהקות.

ניסוח מתמתי של רווח סמך הוא: 

בהינתן מדגם $\mathcal{D}=\{x^{(i)}\}$ מהתפלגות $F_{\theta}$ הידועה למעט ערכו של הפרמטר $\theta$, רווח סמך בעל רמת מובהקות $\alpha$ הוא קטע שקצוותיו מחושבים על פי המדגם כך שההסתברות של הפרמטר $\theta$ להיות בתוך קטע זה היא $1-\alpha$.

**משפט 1 - בניסוח רווח סמך** על ידי השוואת אגף ימין ל-$\delta$, כלומר בחירת $\varepsilon=\frac{1}{N} \log\frac{|H|}{\delta}$, ניתן לקבל את הצורה הבאה של המשפט, כאשר הפרמטר $\delta$ נקרא רווח הסמך:

- לכל $\delta>0$ מתקיים בהתסברות של $(1-\delta)$ לפחות $L(\hat{h}_{\mathcal{D}}) < \frac{1}{N} \log\frac{|H|}{\delta}$

**משפט 1 - ניסוח סיבוכיות המדגם** החסם שקיבלנו מאפשר לנו לבחור את גודל המדגם $N$ המבטיח שגיאה קטנה כרצוננו, ובהתסברות גבוה כרצוננו, 

- אם $N>\frac{1}{\varepsilon} \log\frac{|H|}{\delta}$, נקבל כי $L(\hat{h}_{\mathcal{D}})$ בהסתברות $(1-\delta$) לפחות. 


 מספר מונחים בסיסיים בלמיהדה חישובית:

 אלגוריתם כלשהו לבחירת $\hat{h}_{\mathcal{D}} \in H$  שעבורו  $P(L(\hat{h}_{\mathcal{D}})>\varepsilon) \rightarrow 0$ כאשר  $N \rightarrow \infty$ (לכל $ h_0 \in H $ ) נקרא אלגוריתם PAC – Probably Approximately Correct. קבוצת השערות  $H$ שעבורה קיים אלגוריתם PAC נקראת ברת-למידה (Learnable). 

משפט 1 מראה כי האלגוריתם הממזער את השגיאה האמפירית היא אלגוריתם PAC עבור כל קבוצת השערות סופית  (ולפיכך וכי כל קבוצת השערות סופית היא ברת למידה). יתר על כן, בהמשך נראה (בעזרת אותו אלגוריתם) כי כל קבוצת השערות בעלת מימד VC (גודל שיוגדר בהמשך) סופי היא ברת למידה. 

  נעבור כעת למקרה הכללי יותר שבו פונקצית המטרה $h_0$  אינה כלולה בהכרח בקבוצת ההשערות $H$ , ולמעשה איננו מניחים הנחה כלשהי לגביה. במקרה זה $L^* \neq 0$.   

**משפט 2** נניח כי $|H| < \infty$ ונסמך שוב $L^*=\underset{h \in H}{\min}L(h)$. אזי, לכל $\varepsilon > 0$ 

 $$P(L(\hat{h}_{\mathcal{D}})>L^* + \varepsilon) < 2|H|e^{-\frac{1}{2}\varepsilon^2 n}$$

הערות:

- ניתן לראות כי חסם זה חלק מהקודם, כיוון שקצה הדעיכה המעריכי של הסתברות הטעות הינו $\varepsilon^2$

- ניסוח רווח סמך עבור משפט זה הוא - $L(\hat{h}_{\mathcal{D}}) < L^* + \sqrt{\frac{2}{N}\log\frac{2|H|}{\delta}}$ בהסתברות $1-\delta$ לפחות. האיבר הראשון ($L^*$) מבטא את שגיאת הקירוב, אותה אי אפשר למזער, והשני את שגיאת השערוך. 

- מהי סיבוכיות המדגם?

על מנת להוכיח את המשפטים נגדיר את ההגדרות הבאות: 

אוסף ההשערות ב-$H$ העקביות עם הנתונים מוגדר להיות ה-**version space**. אוסף השערות זה מוגדר בצורה הבאה

$$VS_{H} = \{h_i \in H : \hat{L}_{\mathcal{D}}(h^{(i)})=0, i=1,2,...,|H| \}$$

עבור אלגוריתם ההמזער את השגיאה האמפירית ידוע כי מתקיים $\hat{h}_{\mathcal{D}} \in VS_H$.

אוסף ההשערות הרעות ב-$H$ מוגדר בצורה הבאה:

$$B = \{h_i \in H : L(h^{(i)})>\epsilon, i=1,2,...,|H| \}$$

הערות: 

- שימו לב שההשערות העקביות מוגדרות ע"י השגיאה האמפירית, בעוד שההשערות הרעות מוגדרות ע"י השגיאה ה"אמיתית".

- הקבוצה $B$ אינה אקראית, כלומר אינה תלויה במדגם. 

- ככל שגודל המדגם גדל, הקבוצה $VS_H$ (התלויה במדגם) קטנה.

אנו מעוניינים להעריך את ההסתברות שקיימת השערה רעה שהיא עקבית, כלומר, $h\in(VS_H \cap B)$. 

בשביל ההוכחה נצטרך את חסם האיחוד (union bound) שהוא

$$P(\cup_{i=1}^NA_i)\le\sum_{i=1}^N P(A_i) \le N \underset{1 \le i \ge N}{\max}P(A_i)$$

כאשר שוויון מתקיים אם המאורעות זרים.

**הוכחת משפט 1**

נתבונן בהשערה מסוימת $h_i$ כך שמתקיים

$$P(h_i(x^{(i)})=y^{(i)} \quad\text{and}\quad h_i \in B) < 1-\varepsilon$$

נשים לב שההסתברות היא רק ביחס למשתנה האקראי $x^{(i)}$ כאשר אנו מגבילים את עצמו ל-$h_i \in B$ שכן $B$ היא קבוצה לא אקראית. 

בגלל שהדגימות i.i.d מתקיים 

$$P(h_i \in (VS_H \cap B)) < (1-\varepsilon)^N$$

נגדיר את $h_i \in (VS_H \cap B)$ להיות המאורע $A_i$ ונשתמש בחסם האיחוד כך שנקבל 

$$P(\exist h_i \in (VS_H \cap B) \le |B|(1-\varepsilon)^N)$$

הגודל של הקבוצה $B$ אינו ידוע ולכן נרשום

$$P(\exist h_i \in (VS_H \cap B) \le |H|(1-\varepsilon)^N) \le |H|e^{-\varepsilon N}$$

כאשר האי שוויון האחרון נובע מתוך $1-\varepsilon \le e^{-\varepsilon}$. 

וכך הוכחנו את משפט 1. 

**הוכחת משפט 2** 

בשביל להוכיח את משפט 2, ראשית נגדיר את אי שוויון Hoeffding. 

יהי $\{Z^{(i)}\}_{i=1}^N$ משתנים אקראים i.i.d המוגבלים בקטע סופי $a \le Z^{(i)} \le b$ אזי 

$$P(|\frac{1}{N}\sum_{i=1}^N(Z^{(i)}-E(Z^{(i)}))|>\varepsilon) \le 2 \exp \biggl(- \frac{2N\varepsilon^2}{(b-a)^2}  \biggr)$$

לחסם זה יש קצב מעריכי. 

שימו לב שחסם זה מתעלם משונות המשתנה האקראי, ניתן לקחת אותה בחשבון לצורך שיפור החסם. 

מטרתנו בהוכחה זאת היא לחסום את $P(L(\hat{h}_{\mathcal{D}})-L^* > \varepsilon)$. לשם כך נשתמש באי השוויונות הבאים:

$$L(\hat{h}_{\mathcal{D}})-L^* < 2 \underset{h \in H}{\max}|L(h)-L(\hat{h}_{\mathcal{D}})|$$

לשם פשטות נניח כי קיים $h^* \in H$ כך שמתקיים $L^*=L(h^*)$ אזי 

$$
L(\hat{h}_{\mathcal{D}})-L^* =
L(\hat{h}_{\mathcal{D}}) - L(\hat{h}_{\mathcal{D}}) + L(\hat{h}_{\mathcal{D}}) - L^* \le
[L(\hat{h}_{\mathcal{D}}) - L(\hat{h}_{\mathcal{D}})] + [L(h^*) - L^*] \le
2 \underset{h \in H}{\max}|L(h)-L(\hat{h}_{\mathcal{D}})|
$$

כעת, נרצה להשתמש בחסם Hoeffding. לשם כך נשים לב כי מתקיים: 

$$L_{\mathcal{D}}(\hat{h})=\frac{1}{N}\sum_{i=1}^N Z^{(i)}, \quad Z^{(i)}=I\{h(x^{(i)}) \neq y^{(i)} \}, \quad E(Z^{(i)}) = L(h) $$

כעת נציב בחסם Hoeefding עם $a=0, b=1, \frac{\varepsilon}{2}$ ונקבל 

$$
P(|L(h)-L(\hat{h}_{\mathcal{D}})|) > \frac{\varepsilon}{2}) \le 2 |H| \exp \biggl(-N \frac{\varepsilon^2}{2}  \biggr)
$$

סה"כ, נוכל להשתמש באי השוויונות שהוכחנו ובחסם האיחוד כך שנקבל

$$
P(|L(\hat{h}_{\mathcal{D}})-L^*|) > \varepsilon) \le
P\biggl(\underset{h \in H}{\max}|L(h)-L(\hat{h}_{\mathcal{D}})| > \frac{\varepsilon}{2}\biggr) \le 
|H|\underset{h \in H}{\max}P\biggl(|L(h)-L(\hat{h}_{\mathcal{D}})|\frac{\varepsilon}{2}\biggr) \le 
2 |H| \exp(-N\frac{\varepsilon^2}{2})
$$

והוכחנו את המשפט השני. 

**מגבלות החסמים שפותחו:**

ראינו חסם מהצורה הבאה, $L(\hat{h}_{\mathcal{D}}) < L^* + \sqrt{\frac{2}{N}\log\frac{2|H|}{\delta}}$,  בהסתברות  $(1-\delta)$ לפחות. 

אנו יכולים לפרש את האיבר השני כאיבר המודד את מורכבות מחלקת ההשערות – במקרה זה מורכבות נמדדת ע"ס גודל הקבוצה. 

אבל חסם זה אינו תלוי בפילוג הדוגמאות, במדגם והוא ספציפי לאלגוריתם מזעור השגיאה האמפירית. 

מקור עוצמתו הוא גם מקור חולשתו, שכן הוא מטפל במקרה הגרוע ביותר ואינו מנצלים את המבנה של בעיה נתונה. חסמים משופרים קיימים היום,  אך קשים להוכחה במידה ניכרת. חסמים אלה הם מהצורה

בהסתברות גדולה מ $1-\delta$ , אלגוריתם נתון (לאו בהכרח מזעור שגיאה אמפירית) הבוחר השערה $\hat{h}_{\mathcal{D}}$  מקיים 
 
$$L(\hat{h}_{\mathcal{D}}) < L^* + \Omega(\hat{h}_{\mathcal{D}}, \mathcal{D}, H)$$

כאשר  $\Omega(\hat{h}_{\mathcal{D}}, \mathcal{D}, H)$ איבר מורכבות הדועך לאפס עבור $n \rightarrow \infty$.


</div>
